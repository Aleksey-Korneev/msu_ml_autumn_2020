{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1***\n",
    "$$ \\mathop{\\nabla}_{w} Q_{r}(w_{old}) = \\mathop{\\nabla}_{w} ( \\frac{1}{n} \\sum_{i=1}^n (\\langle x_{i}, w_{old}\\rangle - y_{i})^2 + \\frac{1}{C} \\sum_{i=1}^D w_{old_{i}}^2 ) = \\frac{2}{n} * \\sum_{i=1}^n (\\langle x_{i}, w_{old} \\rangle - y_{i})x_{i} + \\frac{2}{C} * w_{old} $$\n",
    "$$ w_{new} = w_{old} - \\alpha * \\mathop{\\nabla}_{w} Q_{r}(w_{old}) = w_{old} * (1 - \\frac{2*\\alpha}{C}) - \\frac{2*\\alpha}{n} * \\sum_{i=1}^n (\\langle x_{i}, w_{old} \\rangle - y_{i})x_{i} $$\n",
    "\n",
    "Некоторые соображения:\n",
    "1) Градиент изменяет вектор весов таким образом, чтобы уменьшить ошибку предсказания алгоритма, на каждом объекте обучающей выборки (благодаря знаку выражения $ \\langle x_{i}, w_{old} \\rangle - y_{i} $).\n",
    "\n",
    "2) Чем больше положительная константа $ C $, тем сильнее значение $ w_{old} $ учитывается при вычислении $ w_{new} $.\n",
    "\n",
    "3) Чем больше положительная константа $ \\alpha $, тем сильнее значение антиградиента учитывается при вычислении $ w_{new} $.\n",
    "\n",
    "-------------------------\n",
    "***2***\n",
    "$$ a_i = a(x_i) = \\sigma( \\langle\\,x_i,w\\rangle ) = \\frac{1}{1 + \\exp(-\\langle\\,x_i,w\\rangle)} $$\n",
    "\n",
    "$$ \\frac{d\\sigma(\\langle\\,x_i,w\\rangle)}{dw} = \\frac{\\exp(-\\langle\\,x_i,w\\rangle) * x_i}{(1 + \\exp(-\\langle\\,x_i,w\\rangle))^2} = \\sigma (1 - \\sigma) x_i $$\n",
    "\n",
    "$$ L(w) = - \\frac{1}{n}\\left[\\sum_{i=1}^n y_i \\log \\sigma( \\langle\\,x_i,w\\rangle ) + ( 1 - y_i) \\log (1 - \\sigma( \\langle\\,x_i,w\\rangle )) \\right] + \\frac{1}{C}\\sum_{i=1}^{D}w_j^2 $$\n",
    "\n",
    "$$ \\nabla_wL(w) = - \\frac{1}{n}\\left[\\sum_{i=1}^n y_i \\frac{\\sigma (1 - \\sigma) x_i}{\\sigma} - ( 1 - y_i) \\frac{\\sigma (1 - \\sigma) x_i}{1 - \\sigma}) \\right] + \\frac{1}{C} = - \\frac{1}{n}\\left[\\sum_{i=1}^n (y_i(1 - {\\sigma})x_i + (1 - y_i)(- {\\sigma})x_i) \\right] + \\frac{1}{C} = \\frac{1}{n}\\left[\\sum_{i=1}^n (a_i - y_i)x_i \\right] + \\frac{2}{C}w $$\n",
    "\n",
    "$$ w_{new} = w_{old} - {\\alpha}\\nabla_wL(w_{old}) = w_{old} - \\alpha * \\mathop{\\nabla}_{w} Q_{r}(w_{old}) = w_{old} * (1 - \\frac{2*\\alpha}{C}) - \\frac{\\alpha}{n} * \\sum_{i=1}^n (\\frac{1}{1 + \\exp(-\\langle\\,x_i,w\\rangle)} - y_{i})x_{i} $$\n",
    "\n",
    "Размышления аналогичны приведенным в расчетах линейной регрессии.\n",
    "\n",
    "--------------\n",
    "***3***\n",
    "$$ L(w) = 1/2 * \\| \\langle X, w \\rangle - y \\| $$\n",
    "\n",
    "$$ \\nabla_wL(w) = X^T{}X w - X^T y $$\n",
    "\n",
    "$$ \\nabla_{w}^2{}L(w) = X^T{}X $$\n",
    "\n",
    "$$ \\forall z \\in R^n, z \\ne 0 : z^T{}X^T{}X{}z = \\|Xz\\| \\gt 0 \\Rightarrow $$ Гессиан положительно определен, т.к X имеет полный ранг, содержит строк не меньше, чем столбцов.\n",
    "\n",
    "$$ w = (X^T{}X + \\frac{1}{C}I)^{-1}X^TY $$ - точное решение, локальный минимум. Дополнительное слагаемое $ \\frac{1}{C}I $ позволяет избавиться от линейной зависимости строк $ X $.\n",
    "\n",
    "------------\n",
    "***4***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
